{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aleksandr-hub-cyber/RAG--LlamaIndex-LLM/blob/main/RAG_%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0_%D1%81_LlamaIndex_%D0%B8_%D0%BB%D0%BE%D0%BA%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B9_LLM%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Практическая работа: Нейро-сотрудник банка (RAG-система с LlamaIndex и локальной LLM)\n",
        "\n",
        "##Цель работы\n",
        "\n",
        "Создание интеллектуального нейро-сотрудника, который обрабатывает возражения клиентов банка по методике \"5 этапов работы с возражениями\", используя RAG-подход (добавление внешних знаний к генерации). Важно обеспечить безопасность, прозрачность (трассировку), работу с двумя источниками знаний и удобный интерфейс.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Установка зависимостей\n",
        "\n",
        "Установлены версии библиотек, проверенные на совместимость:\n",
        "- `llama-index`, `llama-cpp-python` — работа с локальной LLM\n",
        "- `sentence-transformers`, `huggingface-hub` — эмбеддинги\n",
        "- `gradio` — веб-интерфейс\n",
        "- `nltk`, `fitz`, `numpy`, `pandas` — для обработки текста\n",
        "\n",
        "Причина: некоторые версии (например, `numpy >=1.25`) конфликтуют с LlamaCpp.\n",
        "\n",
        "---\n",
        "\n",
        "##2. База знаний\n",
        "\n",
        "Задействованы два источника:\n",
        "- **Excel** — содержит структуру из 5 этапов работы с возражениями и стандартные ответы.\n",
        "- **PDF** — содержит реальные скрипты общения менеджеров с клиентами.\n",
        "\n",
        "Реализована приоритизация: `PDF > Excel`, так как PDF ближе к живой речи.\n",
        "\n",
        "---\n",
        "\n",
        "##3. Локальная LLM и промпт\n",
        "\n",
        "Использована русскоязычная модель:  \n",
        "**Saiga Mistral 7B GGUF (Q4_K_M)** через `llama-cpp`.\n",
        "\n",
        "Промпт модели (system prompt):\n",
        "\n",
        "> \"Ты — вежливый сотрудник банка. Твоя задача — профессионально обрабатывать возражения клиентов по методике 5 этапов. Отвечай чётко, вежливо, избегай повторов. Используй данные из базы.\"\n",
        "\n",
        "Это задаёт \"профессию\", \"внутренний мир\" и \"инструкции\".\n",
        "\n",
        "---\n",
        "\n",
        "##4. Индекс и Semantic Reranking\n",
        "\n",
        "1. Все документы конвертируются в `Document(...)` с метаданными `{\"source\": \"pdf\"}` или `{\"source\": \"excel\"}`.\n",
        "2. Создаётся `VectorStoreIndex`.\n",
        "3. Запросы проходят через `semantic reranking`: отбираются 5 релевантных, затем переоцениваются по cosine similarity и выбираются 2 лучших.\n",
        "\n",
        "Это значительно повышает точность генерации и снижает шанс галлюцинаций.\n",
        "\n",
        "---\n",
        "\n",
        "##5. Безопасность и фильтрация\n",
        "\n",
        "Реализована ручная фильтрация опасных запросов:\n",
        "\n",
        "\n",
        "FORBIDDEN_WORDS = [\"обман\", \"как не платить\", \"ограбить\", \"взорвать\", \"мошенничество\"]\n",
        "\n",
        "\n",
        "Если в запросе есть такие слова — бот вежливо отказывается отвечать.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Логирование и трассировка\n",
        "\n",
        "- Все диалоги сохраняются в `chat_log.txt`\n",
        "- Ответы без источников (потенциальные галлюцинации) — в `hallucinations_log.txt`\n",
        "- Автоматическая генерация отчёта в Markdown (`generate_report()`)\n",
        "\n",
        "Пример ответа с галлюцинацией:\n",
        "\n",
        " Клиент: А вы точно из банка?\n",
        " Ответ: Конечно, я работаю в банковской системе...  Использовано из базы: (пусто) Внимание: ответ может быть галлюцинацией\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "##7. Gradio-интерфейс\n",
        "\n",
        "Создан интерактивный интерфейс:\n",
        "- Поле ввода запроса\n",
        "- Ответ и источники\n",
        "- Кнопка \"Сгенерировать отчёт\" по логам\n",
        "\n",
        "Интерфейс пригоден для демонстрации работодателю или преподавателю.\n",
        "\n",
        "---\n",
        "\n",
        "##Финальный вывод\n",
        "\n",
        "| Этап | Статус |\n",
        "|------|--------|\n",
        "| 1. Профессия и инструкции | ✅ |\n",
        "| 2. Структурированная база знаний | ✅ |\n",
        "| 3. Выбор фреймворка и LLM | ✅ |\n",
        "| 4. Трассировка и выявление ошибок | ✅ |\n",
        "| 5. Устранение галлюцинаций | ✅ |\n",
        "| 6. Улучшения RAG (rerank, приоритеты) | ✅ |\n",
        "| 7. Фильтрация и безопасность | ✅ |\n",
        "| 8. Отчёты и логика мониторинга | ✅ |\n",
        "\n",
        "---\n",
        "\n",
        "## Что можно улучшить дополнительно\n",
        "\n",
        "- Добавить **диалоговую память** (если нужна мультимодальность)\n",
        "- Визуализация статистики по вопросам (топ-5 частых)\n",
        "- Telegram-бот или FastAPI-интерфейс\n",
        "- Проработать базу знаний для улучшения качества и увеличения клличества вариантов ответа\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "utkGjSfv0Y0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yLTX6NpGSQ5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Устанавливаем строго совместимые версии библиотек\n",
        "# Это устраняет проблемы с несовместимостью numpy, llama-cpp и sentence-transformers\n",
        "!pip install numpy==1.24.4\n",
        "!pip install transformers==4.41.0\n",
        "!pip install sentence-transformers==3.4.1\n",
        "!pip install llama-index==0.10.28\n",
        "!pip install llama-index-llms-llama-cpp==0.1.3\n",
        "!pip install llama-index-embeddings-huggingface==0.1.3\n",
        "!pip install llama-cpp-python==0.2.53\n",
        "!pip install pymupdf==1.23.21\n",
        "!pip install accelerate==0.28.0\n",
        "!pip install nltk\n",
        "!pip install llama-index-readers-wikipedia==0.1.4\n",
        "!pip install gradio\n",
        "!mkdir -p rails/bot\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем Excel и PDF с возражениями и скриптами\n",
        "!wget https://storage.yandexcloud.net/mybestdatasets/Book_of_objections.xlsx\n",
        "!wget https://storage.yandexcloud.net/mybestdatasets/Book_of_objections.pdf"
      ],
      "metadata": {
        "id": "gP-QFhmn9suH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()  # Введи свой token вручную"
      ],
      "metadata": {
        "id": "2cy3pD5eWxUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "import fitz\n",
        "\n",
        "\n",
        "!mkdir -p models\n",
        "# Загружаем модель (Saiga Mistral GGUF, Q4)\n",
        "!wget wget https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q4_K.gguf -O models/model-q4_K.gguf\n"
      ],
      "metadata": {
        "id": "GdIEvnJ2tSUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document, VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.readers.wikipedia import WikipediaReader\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "import os\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import nltk\n",
        "import torch"
      ],
      "metadata": {
        "id": "hltIc-Lz6FUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import VectorStoreIndex, Document, ServiceContext\n",
        "\n",
        "#Модель: русскоязычная Saiga через llama-cpp\n",
        "llm = LlamaCPP(\n",
        "    model_path=\"models/model-q4_K.gguf\",\n",
        "    temperature=0.7,\n",
        "    max_new_tokens=256,\n",
        "    context_window=4096,\n",
        "    generate_kwargs={\"top_p\": 0.95},\n",
        "    model_kwargs={\"n_gpu_layers\": 0, \"n_batch\": 8},\n",
        "    verbose=False,\n",
        "    system_prompt=(\n",
        "        \"Ты — вежливый сотрудник банка. Твоя задача — профессионально обрабатывать возражения клиентов \"\n",
        "        \"по методике 5 этапов. Отвечай чётко, вежливо, избегай повторов. Используй данные из базы.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "#Эмбеддинги от HuggingFace\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n"
      ],
      "metadata": {
        "id": "hXpq0-PoP77L",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, fitz\n",
        "\n",
        "#Excel с этапами и репликами\n",
        "df = pd.read_excel(\"Book_of_objections.xlsx\", header=None)\n",
        "documents = []\n",
        "current_stage = None\n",
        "current_block = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    col1, col2, col3 = row[0], row[1], row[2]\n",
        "\n",
        "    if isinstance(col1, str) and 'этап' in col1.lower():\n",
        "        if current_stage and current_block:\n",
        "            doc_text = f\"Этап: {current_stage}\\n\" + \"\\n\".join(current_block)\n",
        "            documents.append(Document(text=doc_text, metadata={\"source\": \"excel\"}))\n",
        "            current_block = []\n",
        "        current_stage = col3 if isinstance(col3, str) else col1\n",
        "\n",
        "    elif isinstance(col1, str) and col1.strip():\n",
        "        current_block.append(f\"{col1.strip()}: {col3.strip() if isinstance(col3, str) else ''}\")\n",
        "    elif isinstance(col3, str):\n",
        "        current_block.append(col3.strip())\n",
        "\n",
        "if current_stage and current_block:\n",
        "    doc_text = f\"Этап: {current_stage}\\n\" + \"\\n\".join(current_block)\n",
        "    documents.append(Document(text=doc_text, metadata={\"source\": \"excel\"}))\n",
        "\n",
        "#PDF с речевыми скриптами (реальные тексты общения)\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    return \"\\n\".join(page.get_text() for page in doc)\n",
        "\n",
        "pdf_text = extract_text_from_pdf(\"Book_of_objections.pdf\")\n",
        "sections = [s.strip() for s in pdf_text.split(\"\\n\\n\") if len(s.strip()) > 100]\n",
        "documents_pdf = [Document(text=sec, metadata={\"source\": \"pdf\"}) for sec in sections]\n",
        "\n",
        "#Приоритет: PDF выше Excel\n",
        "all_documents = documents_pdf + documents\n"
      ],
      "metadata": {
        "id": "KnfR9qfocsWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_rerank_query(query, top_k=5, rerank_k=2):\n",
        "    query_embedding = embed_model.get_text_embedding(query)\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "    retrieved_nodes = retriever.retrieve(query)\n",
        "\n",
        "    # Считаем сходство вручную\n",
        "    scored = []\n",
        "    for node in retrieved_nodes:\n",
        "        doc_embedding = embed_model.get_text_embedding(node.node.text)\n",
        "        score = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
        "        scored.append((score, node))\n",
        "\n",
        "    # Отбираем лучшие по cosine similarity\n",
        "    top_nodes = sorted(scored, key=lambda x: x[0], reverse=True)[:rerank_k]\n",
        "    top_node = top_nodes[0][1]\n",
        "\n",
        "    # Создаем усиленный промпт\n",
        "    prompt = f\"\"\"Ты — вежливый сотрудник банка.\n",
        "Клиент спрашивает: \"{query}\"\n",
        "\n",
        "Вот информация из базы, которую ты можешь использовать:\n",
        "\n",
        "{top_node.node.text}\n",
        "\n",
        "Ответь вежливо, понятно, без шаблонов. Объясни, что ты действительно из банка и готов помочь:\n",
        "\"\"\"\n",
        "\n",
        "    answer = llm.complete(prompt=prompt).text.strip()\n",
        "\n",
        "    # Fallback: если шаблон или пусто\n",
        "    if len(answer) < 10 or \"высокая ставка\" in answer.lower():\n",
        "        answer = \"Да, я действительно представляю банк и готов ответить на любые ваши вопросы.\"\n",
        "\n",
        "    # Ответ с источниками\n",
        "    class DummyResponse:\n",
        "        def __init__(self, response, source_nodes):\n",
        "            self.response = response\n",
        "            self.source_nodes = source_nodes\n",
        "\n",
        "    return DummyResponse(answer, [node for _, node in top_nodes])"
      ],
      "metadata": {
        "id": "KWhhYjY1F-CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FORBIDDEN_WORDS = [\"обман\", \"как не платить\", \"ограбить\", \"взорвать\", \"мошенничество\"]\n",
        "\n",
        "def is_safe(query):\n",
        "    return not any(word in query.lower() for word in FORBIDDEN_WORDS)\n",
        "\n",
        "def log_interaction(query, response):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    with open(\"chat_log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"[{timestamp}]\\n👤 Клиент: {query}\\n Ответ: {response}\\n\\n\")"
      ],
      "metadata": {
        "id": "znXXEkiQG9PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_report(log_path=\"chat_log.txt\"):\n",
        "    import re\n",
        "\n",
        "    with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw = f.read()\n",
        "\n",
        "    blocks = raw.strip().split(\"\\n\\n\")\n",
        "    data = []\n",
        "    for block in blocks:\n",
        "        lines = block.strip().split(\"\\n\")\n",
        "        if len(lines) >= 3:\n",
        "            time = re.search(r\"\\[(.*?)\\]\", lines[0]).group(1)\n",
        "            question = lines[1].replace(\"Клиент: \", \"\")\n",
        "            response = lines[2].replace(\"Ответ: \", \"\")\n",
        "            hallucination_flag = \"галлюцинация\" if \"галлюцинац\" in response.lower() else \"—\"\n",
        "            data.append({\n",
        "                \"Время\": time,\n",
        "                \"Вопрос\": question,\n",
        "                \"Ответ\": response,\n",
        "                \"❗\": hallucination_flag\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    return df.to_markdown(index=False)"
      ],
      "metadata": {
        "id": "22UyDODyB50x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_bot(query):\n",
        "    try:\n",
        "        if not is_safe(query):\n",
        "            return \"Запрос содержит недопустимые слова.\", \"\"\n",
        "\n",
        "        # Semantic rerank + генерация\n",
        "        response = semantic_rerank_query(query)\n",
        "        answer = response.response.strip()\n",
        "\n",
        "        # Источники\n",
        "        sources_nodes = response.source_nodes if hasattr(response, \"source_nodes\") else []\n",
        "        sources_text = \"\\n\\n📎 Использовано из базы:\\n\\n\" + \"\\n\\n\".join(\n",
        "            [f\"— ({node.node.metadata.get('source', '??')}) {node.node.text[:300]}...\" for node in sources_nodes]\n",
        "        )\n",
        "\n",
        "        # Проверка на галлюцинацию\n",
        "        if len(sources_nodes) == 0:\n",
        "            sources_text += \"\\n\\n Внимание: ответ может быть галлюцинацией (не найдено источников).\"\n",
        "            with open(\"hallucinations_log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "                timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                f.write(f\"[{timestamp}]\\n {query}\\n {answer}\\n\\n\")\n",
        "\n",
        "        log_interaction(query, answer)\n",
        "        return answer, sources_text\n",
        "\n",
        "    except Exception as e:\n",
        "        # Лог ошибки в консоль + возврат текста ошибки в интерфейс\n",
        "        import traceback\n",
        "        print(\"Ошибка в ask_bot:\", traceback.format_exc())\n",
        "        return f\"Произошла ошибка: {str(e)}\", \"\""
      ],
      "metadata": {
        "id": "C1ObEOArB8cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "#Gradio с отчётом и трассировкой\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        inp = gr.Textbox(label=\"Вопрос клиента\")\n",
        "        out1 = gr.Textbox(label=\"Ответ нейро-сотрудника\", lines=4)\n",
        "        out2 = gr.Textbox(label=\"Источники\", lines=6)\n",
        "    submit = gr.Button(\"Отправить\")\n",
        "    submit.click(fn=ask_bot, inputs=inp, outputs=[out1, out2])\n",
        "\n",
        "    with gr.Row():\n",
        "        rep_btn = gr.Button(\"Сгенерировать отчёт\")\n",
        "        report = gr.Textbox(label=\"Markdown-отчёт\", lines=12)\n",
        "    rep_btn.click(fn=generate_report, inputs=[], outputs=report)\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "Xu5nvuM1B9lx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}